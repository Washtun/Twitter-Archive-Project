{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reporting: Dataset  Information and Wrangling Process for WeRateDog Twitter Archive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Project: WeRateDogs\n",
    "\n",
    "## Table of Contents\n",
    "<ul>\n",
    "<li><a href=\"#intro\">Introduction</a></li>\n",
    "<li><a href=\"#wrangling\">Data Wrangling</a></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='intro'></a>\n",
    "## Introduction\n",
    "\n",
    "### Dataset Description \n",
    "\n",
    "In this project, we work on the three datasets;\n",
    "\n",
    "`Enhanced Twitter Archive`\n",
    "This is a WeRateDogs Twitter archive dataset which contains basic tweet data for all 5000+ of their tweets, but not everything. One column the archive does contain though: each tweet's text, which was used to extract rating, dog name, and dog \"stage\" (i.e. doggo, floofer, pupper, and puppo) to make this Twitter archive \"enhanced.\" Of the 5000+ tweets, we filtered for tweets with ratings only (there are 2356). This file is read into pandas as rating_puppies using the 'twitter-archive-enhanced.csv' provided by WeRateDogs.\n",
    "\n",
    "\n",
    "`The tweet image predictions`\n",
    "This file is given as (image_predictions.tsv), hosted on Udacity's servers and it is downloaded programmatically using the `Requests` library from the following [URL]:(https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv). This image file is gotten when every image in the WeRateDogs Twitter archive is ran through a `neural network` that can classify **breeds of dogs**. This gives a table full of image predictions (the top three only) alongside each tweet ID, image URL, and the image number that corresponded to the most confident prediction (numbered 1 to 4 since tweets can have up to four images). The columns in this file include; `tweet_id`, `img_num`, `jpg_url`, `p1`, `p1_conf`, `p1_dog`,`p2`, `p2_conf`, `p2_dog`, `p3`, `p3_conf`, and `p3_dog`, where;\n",
    "- tweet_id is the last part of the tweet URL after \"status/\" \n",
    "- p1 is the algorithm's #1 prediction for the image in the tweet → golden retriever\n",
    "- p1_conf is how confident the algorithm is in its #1 prediction → 95%\n",
    "- p1_dog is whether or not the #1 prediction is a breed of dog → TRUE\n",
    "- p2 is the algorithm's second most likely prediction → Labrador retriever\n",
    "- p2_conf is how confident the algorithm is in its #2 prediction → 1%\n",
    "- p2_dog is whether or not the #2 prediction is a breed of dog → TRUE etc.\n",
    "\n",
    "\n",
    "`Additional data from the Twitter API`\n",
    "We gathered each tweet's retweet count and favorite (\"like\") count at the minimum and any additional data we find interesting. Using the tweet IDs in the WeRateDogs Twitter archive, by querying the Twitter API for each tweet's `JSON` data wiyh Python's `Tweepy` library and store each tweet's entire set of JSON data in a file called `tweet_json.txt` file.\n",
    "\n",
    "Each tweet's `JSON` data were written to its own line. Then we read this `tweet_json.txt` file line by line into a pandas DataFrame with (at minimum) tweet ID, retweet count, and favorite count.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='wrangling'></a>\n",
    "## Data Wrangling\n",
    "\n",
    "All three dataset were read into pandas, assessed visually and programmatically, 8 quality issues were detected, and 2 tidiness issues were discovered. These issues are;\n",
    "\n",
    "### Quality issues\n",
    "\n",
    "#### `rating_puppies` table\n",
    "1. Five columns having large amount of missing values in the rating_puppies table.\n",
    "\n",
    "2. The `expanded_urls ` column looks like a repeat of the `tweet_id` column.\n",
    "\n",
    "3. `Source` column is an HTML tag with the required content.\n",
    "\n",
    "4. Erroneous datatype for `Timestamp` \n",
    "\n",
    "5. Nulls represented as (None) in doggo, floofer,pupper, and puppo columns\n",
    "\n",
    "6. The `name` column with \"very\" aren't dogs judging from the `text`column, and the name 'O' is actually O'Malley.\n",
    "\n",
    "#### `image_predictions` table\n",
    "\n",
    "7. Mixture of Uppercase letter and lowercase used for the first character of values in columns `p1`,`p2`,and `p3`\n",
    "\n",
    "#### `additional_tweets_df` table\n",
    "\n",
    "8. Missing records in the tweets_df table (29)\n",
    "\n",
    "\n",
    "### Tidiness issues\n",
    "1. Timestamp column with combined information for the Date, Day and Time of tweet.\n",
    "\n",
    "2. image_prediction tables containing 3 seperate predictions.\n",
    "\n",
    "\n",
    "#### Cleaining The Issues\n",
    "These issues were resolved one after the other through the following steps;\n",
    "\n",
    "- Dropping these columns with pandas.drop() function because they aren't needed for analysis\n",
    "\n",
    "- Dropping the `expanded_urls` because the last part of the string in the URLs is a repeat of the tweet_id.\n",
    "\n",
    "- Using str function, pandas.Series.extract function and regular expression to extract the content in the tags.\n",
    "\n",
    "- Converting `timestamp` column datatype to datetime64[ns] using pandas to_datetime function.\n",
    "\n",
    "- Extracting the date, year, month, day, and time a tweet was created using pandas.Series.datetime methods. Then Dropping the `timestamp` column\n",
    "\n",
    "- Replacing the 'None' string with NaN values using `np.nan`, and dropping these columns.\n",
    "\n",
    "- Dropping the rows with name as \"very\" and replacing \"O\" with \"O'Malley\". And Capitalizing the values in the name column with `str.title()` function.\n",
    "\n",
    "- Capitalizing the first character of each word using `str.title()` in these columns.\n",
    "\n",
    "- Separating each predictions with pandas DataFrame `drop()` function.\n",
    "\n",
    "- Merging all table together using pandas merge and reduce function.\n",
    "\n",
    "- Saved the gathered, assessed, and cleaned master dataset to a CSV file named \"twitter_archive_master.csv\" using pandas `to_csv()`function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
